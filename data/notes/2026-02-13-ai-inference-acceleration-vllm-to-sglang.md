# AI 推理加速 2026：从 vLLM 到 SGLang 的性能革命

**探索时间：** 2026-02-13
**信息源：**
- The State of LLM Serving in 2026 (Canteen, 2026-01-03)
- LLM Inference Engines: vLLM vs LMDeploy vs SGLang (AIMultiple, 2025)

---

## 核心发现

### 1. 性能差距的本质：架构哲学而非内核优化

在 H100 80GB 上测试 Llama 3.1 8B 的批处理吞吐量：

- **SGLang:** 16,215 tokens/秒
- **LMDeploy:** 16,132 tokens/秒
- **vLLM (FlashInfer):** 12,553 tokens/秒

**关键洞见：** 即使启用相同的 FlashInfer 内核，SGLang 和 LMDeploy 仍比 vLLM 领先 **29%**。这表明瓶颈已不再是数学内核，而是引擎的内部编排开销。

vLLM 的优势在于其灵活的插件架构（PagedAttention），但这在 Hopper 架构上付出了代价。SGLang 和 LMDeploy 采用**协同设计**（co-design）方式，将注意力机制与内核假设深度集成，实现了极致优化。

### 2. 内存管理的演进：从分页到层次化

**PagedAttention (vLLM)**：将 KV 缓存内存像操作系统页面一样管理，支持动态分配和高效利用。

**RadixAttention (SGLang)**：利用多请求共享公共前缀（系统提示词、few-shot 示例、对话历史）的特点，使用 Trie 树缓存，GPU/CPU 双层存储实现分层缓存。

**演进路径：** PagedAttention → RadixAttention → HiCache

这不仅仅是优化技术，而是反映了从"通用解决方案"到"场景特定优化"的范式转变。

### 3. 注意力机制的压缩与加速

**演进链条：** 标准 MHA → FlashAttention → MLA (Multi-head Latent Attention) → Sparse Attention

MLA 由 DeepSeek 普及，压缩 key-value 表示，减少内存带宽。每一步都在以通用性换取效率。

这意味着未来 AI 系统可能需要根据应用场景选择不同的注意力变体——而非"一个模型打天下"。

### 4. 编译范式从静态转向动态

所有框架都在向 **torch.compile** 靠拢：
- vLLM V1 默认启用 torch.compile
- SGLang 有活跃的开发分支
- 即使是高度优化的 TensorRT 也在适应

这标志着一个转折点：从手工编写 CUDA 内核，转向编译器驱动的优化，使系统能够自适应新硬件。

### 5. 分布式模式的根本性转变

从传统的张量并行（TP）和流水线并行（PP），转向 **prefill/decode 分离**（disaggregation）：

- **Prefill：** 计算密集型，可以放在高性能 GPU 上
- **Decode：** 内存密集型，可以放在不同硬件上

这种分离允许针对不同阶段使用不同的硬件，随着模型规模增长变得愈发重要。

---

## 技术选择决策树

| 使用场景 | 推荐框架 | 理由 |
|---------|---------|------|
| 本地开发 | Ollama | 最简单的设置 |
| 最大吞吐量 | SGLang | RadixAttention，已在大规模生产中验证 |
| NVIDIA 专用性能 | TensorRT | SM 级别的特定优化 |
| 自定义内核开发 | Triton | DSL 灵活性 |
| 生产部署 | vLLM | 成熟的生态系统，广泛模型支持 |
| 生产部署（H100 优先） | LMDeploy | 99.5% 峰值性能，pip install 即可 |

**生产环境的实际选择：** 通常是 SGLang vs vLLM。SGLang 通过 RadixAttention 提供更好的原始吞吐量，vLLM 拥有更广泛的模型支持和更成熟的生态系统。

---

## 我的分析

### 性能的权衡维度

29% 的性能差距（SGLang/LMDeploy vs vLLM）揭示了一个关键决策维度：

**灵活性 vs 特化**

- **vLLM：** 支持超过 218 种模型架构，跨 NVIDIA/AMD/Intel 硬件。适合快速原型和异构环境。
- **SGLang：** 专为性能而设计，需要专门的团队管理依赖。适合专用推理集群。
- **LMDeploy：** 纯 C++ 引擎，零 Python 开销。安装简单但维护成本高。

这不是"哪个更好"的问题，而是"你的优先级是什么"的问题。对于初创公司，vLLM 的快速迭代能力可能比 29% 的性能更值钱。对于大规模部署，29% 意味着显著的成本节约。

### 硬件特定的优化策略

TensorRT 的做法值得深思：为每个 GPU 架构（Turing、Ampere、Hopper、Blackwell）维护单独的内核实现。这种策略：

- **优点：** 达到通用实现无法匹配的性能
- **缺点：** 维护成本极高，需要持续投入

这预示着 AI 基础设施正在走向"专业化道路"。未来可能出现类似 CUDA 生态的分层：通用框架（vLLM）vs 硬件优化层（TensorRT）vs 自定义内核（Triton）。

### 编译驱动优化的兴起

torch.compile 成为默认选择，标志着一个重要转变：从"人类优化"到"编译器优化"。这类似于从汇编语言到高级编程语言的进化——牺牲一些控制力换取可维护性和硬件适配性。

长期来看，这降低了 AI 系统的开发门槛，但同时也可能创造新的"编译器黑盒"问题：当编译器做出不明智的优化决策时，开发者如何调试？

### 量化的下一站：FP4/MXFP

当前量化格局：
- **FP16：** 通用
- **INT8：** 常见
- **FP8：** 增长中（TensorRT、SGLang、vLLM）
- **INT4：** 本地部署常见（GGML）
- **FP4/MXFP：** 新兴（Triton、TensorRT）

MXFP（混合格式浮点）格式值得关注，提供比整数量化更好的每比特质量。这可能成为平衡性能和精度的下一个标准。

### 开发者体验的隐藏成本

虽然性能数字直观，但 AIMultiple 的 benchmark 揭示了**隐藏的工程成本**：

- SGLang 的 FlashInfer 依赖冲突：解决兼容版本需要 6 小时
- vLLM 启用 FlashInfer 需要特定 PyTorch 版本（2.8 Nightly）
- GPU 内存利用率的"甜蜜点"：0.8 是安全区，0.9 会崩溃

这些"现实世界的坑"在技术文档中很少提及，但可能消耗数天的开发时间。选择框架时，不仅要考虑性能峰值，还要考虑部署的稳定性和可维护性。

---

## 对未来的预判

### 短期（2026-2027）

1. **torch.compile 标准化**：所有主流框架都将默认启用编译驱动优化
2. **Prefill/Decode 分离部署**：成为大规模集群的标准模式
3. **MLA 普及**：更多模型采用压缩 KV 表示

### 中期（2027-2029）

1. **硬件加速器多样化**：不仅是 NVIDIA，AMD、Intel、Apple Silicon 都有优化路径
2. **量子化格式统一**：MXFP 可能成为新的低精度标准
3. **Agentic 推理**：多智能体系统的特殊推理需求推动新框架诞生

### 长期（2029+）

1. **神经形态计算集成**：传统 GPU 之外的新硬件范式
2. **自适应编译**：编译器根据运行时特征动态选择优化策略
3. **推理即服务**：云提供商提供高度优化的推理 API，抽象掉底层框架

---

## 结论

AI 推理加速领域正在从"拼硬件"转向"拼架构"。29% 的性能差距不是通过更快 GPU 解决的，而是通过更好的软件工程——更智能的内存管理、更精心的设计权衡、更深度的硬件特定优化。

对于实践者，这意味着：
1. **不要迷信性能数字**——开发体验和稳定性同样重要
2. **理解你的工作负载**——对话场景 RadixAttention 优势明显，批处理场景差距缩小
3. **关注趋势而非具体实现**——编译驱动、分离部署、压缩注意力是大方向

框架的选择已不再是一个技术问题，而是一个战略问题：你更看重灵活性还是极致性能？快速迭代还是稳定部署？

这背后是一个更深层的问题：在 AI 系统中，我们何时该"自己做"，何时该"让系统来做"？
