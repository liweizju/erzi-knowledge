# INS001: LLM 上下文工程分析

**洞见报告** | 编号：INS001 | 日期：2026-02-15

---

## 核心观点

1. **上下文工程已成为独立的技术领域**：从 Prompt Engineering 演化而来，上下文工程是一个更系统化的学科，涵盖上下文窗口设计、KV Cache 管理、RAG 架构选择、多轮对话记忆管理等，是构建生产级 LLM 应用的核心能力。

2. **长上下文与 RAG 不是替代关系，而是分层组合关系**：长上下文窗口（1M+ tokens）解决了"能放多少"的问题，RAG 解决了"该放什么"的问题。两者结合使用才是最优解——知识库规模和查询场景决定配比。

3. **Prompt Caching 是成本优化的关键杠杆**：可以降低 45-90% 的 API 成本，但需要理解各厂商的缓存策略差异。Agent 场景下，将动态内容放在 prompt 末尾、系统提示单独缓存是最佳实践。

---

## 背景与上下文

### 什么是上下文工程

2025-2026 年，AI 领域的一个重要认知转变是：**Prompt Engineering 正在升级为 Context Engineering**。这不是简单的品牌重塑，而是反映了技术复杂度的实质性提升。

Context Engineering 的定义：设计和优化 LLM 输入上下文的系统化过程，包括指令、检索内容、工具定义、历史记忆等，以提升模型在特定任务上的表现。

Andrej Karpathy 的定义更具实操性："在恰当的时刻，将恰当的信息放入上下文窗口。"Tobi Lutke（Shopify CEO）和 Ankur Goyal 也强调这是构建 AI 系统的核心技能。

### 技术演进的四个阶段

根据 FlowHunt 的分析，上下文工程经历了四个时代：

- **Era 1.0（1990s-2020）**：原始计算时代，机器只能处理结构化输入
- **Era 2.0（2020-至今）**：Agent 智能时代，GPT-3 带来真正的自然语言理解
- **Era 3.0（未来）**：人类级智能，系统可以处理高熵信息
- **Era 4.0（更远未来）**：超人类智能，系统主动构建上下文

我们目前处于 Era 2.0，需要人工精心设计上下文流程。

### 上下文窗口的市场格局

| 模型 | 上下文窗口 | 定价（输入/输出，$/MTok） |
|------|-----------|-------------------------|
| Claude 3.5 Sonnet | 200K | $3 / $15 |
| Claude 3 Opus | 200K | $15 / $75 |
| GPT-4o | 128K | $2.5 / $10 |
| Gemini 1.5 Pro | 2M | 按使用量计费 |
| Gemini 2.0 Flash | 1M | 更低成本 |

**关键趋势**：上下文窗口从 4K → 32K → 128K → 200K → 1M+ 的快速扩张，但成本和延迟也随之上升。

---

## 信息基础（I 编号清单）

### 技术演进维度

**I001** — StreamingLLM（MIT, ICLR 2024）：提出 Attention Sink 概念，通过保留初始 token 的 KV 状态，使有限训练窗口的模型能泛化到无限序列长度，无需微调。在 Llama-2 上实现 4M+ tokens 的稳定生成。来源：https://arxiv.org/abs/2309.17453

**I002** — ACE（Agentic Context Engineering，ICLR 2026）：将上下文视为可进化的 playbook，通过 generation → reflection → curation 模块化流程，实现上下文的持续优化。在 AppWorld 排行榜上匹配顶级 Agent 性能。来源：https://arxiv.org/abs/2510.04618

**I003** — Contextual Retrieval（Anthropic，2024）：通过 Contextual Embeddings 和 Contextual BM25，将检索失败率降低 49%（结合 reranking 达 67%）。核心是在 chunk 前添加解释性上下文。来源：https://www.anthropic.com/engineering/contextual-retrieval

**I004** — Needle in a Haystack 基准测试：用于评估 LLM 在长上下文中检索特定信息的能力。GPT-4 在 >64K tokens 时性能下降，>100K 时急剧下降。模型对上下文开头和结尾的信息检索效果最好。来源：https://github.com/gkamradt/LLMTest_NeedleInAHaystack

**I005** — Long Context vs RAG 评估（NTU/Fudan，2025）：长上下文在问答基准上通常优于 RAG，尤其是 Wikipedia 类问题；但 RAG 在对话类和通用问题查询上有优势。来源：https://arxiv.org/html/2501.01880v1

**I006** — 128K 上下文的 KV Cache 消耗约 40GB HBM（Llama 3.1-70B），成为推理瓶颈。来源：Pure Storage Blog

**I007** — LLM 推理系统浪费 60-80% 的 KV Cache 内存，源于碎片化和过度分配。来源：Introl Blog

**I008** — Microsoft FastGen 可减少 50% 的 KV Cache 内存，同时不牺牲质量。来源：Microsoft Research

### 玩家图谱维度

**I009** — Anthropic Prompt Caching：写入缓存成本 +25%，读取缓存成本仅 10%。Claude 3.5 Sonnet: 写入 $3.75/MTok，读取 $0.30/MTok。来源：https://claude.com/blog/prompt-caching

**I010** — Prompt Caching 效果：100K token 缓存的书籍对话，延迟从 11.5s 降至 2.4s（-79%），成本降低 90%。来源：Anthropic

**I011** — Claude 3 Haiku 的缓存读取成本仅 $0.03/MTok，是最经济的选择。来源：Anthropic Pricing

**I012** — OpenAI、Anthropic、Google 三家 Prompt Caching 策略对比（PwC，2026）：在 DeepResearchBench 上测试，成本降低 45-80%，TTFT 改善 13-31%。来源：https://arxiv.org/html/2601.06007v1

**I013** — Gemini 1.5 Pro 支持 2M token 上下文窗口，约等于 800 万字符、3400 页 Word 文档。来源：Google Cloud

**I014** — 开源长上下文模型推荐（SiliconFlow，2026）：Qwen3-30B-A3B-Thinking-2507、MiniMax-M1-80k、Qwen3-30B-A3B-Instruct-2507。来源：https://www.siliconflow.com/articles/en/the-best-open-source-llm-for-context-enginneering

**I015** — NVIDIA TensorRT-LLM 提供 KV Cache 复用优化，平衡内存增长和重计算成本。来源：NVIDIA Developer Blog

### 用户/客户维度

**I016** — Notion 使用 Prompt Caching 优化 Notion AI，降低成本和延迟。来源：Anthropic Customer Spotlight

**I017** — 开发者在 Reddit 讨论：RAG 对获取上下文相关示例仍然必要，发送 50 万 tokens 每次请求既浪费又昂贵。来源：https://www.reddit.com/r/MachineLearning/comments/1ax6j73/

**I018** — Agent 场景下的上下文管理挑战：多轮对话累积历史超过窗口限制，需要主动管理策略。来源：Maxim AI

**I019** — JetBrains 研究：Agent 上下文快速增长导致成本飙升，但下游任务性能提升有限。观察掩码（Observation Masking）和 LLM 摘要是两种主要管理方法。来源：https://blog.jetbrains.com/research/2025/12/efficient-context-management/

**I020** — SWE-agent 跳过失败重试轮次，OpenHands 包含所有轮次，导致上下文差异巨大。来源：JetBrains Research

### 成本与性能维度

**I021** — Claude 超过 200K tokens 的输入，Sonnet 从 $3/MTok 涨到 $6/MTok，输出从 $15 涨到 $22.50。来源：IntuitionLabs

**I022** — Prompt Caching 的最佳场景：对话代理、编码助手、大型文档处理、详细指令集、Agent 搜索和工具使用。来源：Anthropic

**I023** — 10 轮对话场景下，Prompt Caching 可降低 53% 成本，TTFT 从 ~10s 降至 ~2.5s。来源：Anthropic

**I024** — Spring AI 测试：到第 10 轮对话时，20K tokens 历史被缓存，后续每轮只支付正常成本的 10%。来源：Spring.io

**I025** — 10K token 的 many-shot prompting 场景，缓存后延迟从 1.6s 降至 1.1s（-31%），成本降低 86%。来源：Anthropic

### 架构与方法论维度

**I026** — Context Engineering 的四大支柱（CodeConductor）：Context Composition（组成）、Ranking and Relevance（排序相关性）、Optimization（优化）、Orchestration（编排）。来源：https://codeconductor.ai/blog/context-engineering/

**I027** — 高级上下文工程策略：Context Masking、KV-Cache 前后缀缓存、Summarization、Chunking、分层上下文设计、多步记忆。来源：CodeConductor

**I028** — Anthropic 建议：知识库 < 200K tokens 时，直接放入 prompt 比 RAG 更简单有效。来源：Anthropic Contextual Retrieval

**I029** — RAG 的核心问题：传统方法在 chunking 时丢失上下文，导致检索失败。Contextual Retrieval 通过为每个 chunk 添加解释性前缀解决。来源：Anthropic

**I030** — 长上下文模型的限制：即使有 2M 窗口，也只能处理约 10 份 300-400 页的年度财务报告，对于企业级知识库远远不够。来源：Unstructured.io

**I031** — RAG 的持久价值：数据聚合、实时更新、成本效益、隐私控制、可解释性。来源：Unstructured.io

**I032** — Embedding 模型的上下文限制：虽然 LLM 已达 1M+，但 embedding 模型仍限制在 8K 以内。LongEmbed 推动到 32K。来源：https://arxiv.org/html/2404.12096v1

**I033** — BM25 结合语义搜索可以捕获精确匹配，如错误代码 "TS-999"。来源：Anthropic

**I034** — Attention Sink 模型优化流式应用：多轮对话、持续生成场景。来源：HuggingFace Blog

**I035** — KV Cache 计算公式：Size = 2 × B × S × L × H × D × (Q/8) / (1024³)，其中 B=Batch Size，S=Sequence Length，L=Layers，H=Heads，D=Head Dimension，Q=Bit Precision。来源：BentoML

**I036** — KV Cache 卸载（Offloading）：将非活跃 KV 数据从 GPU 移至 CPU RAM 或 SSD，释放 GPU 资源。NVIDIA 报告可达 14x TTFT 提升。来源：BentoML

**I037** — LLM 推理系列：KV Cache 使注意力机制的计算复杂度从序列长度的二次方降至线性。来源：Sebastian Raschka

**I038** — Agent 多轮对话评估维度：任务完成度、响应质量、用户体验、记忆与上下文保持、规划与工具集成。来源：https://arxiv.org/abs/2503.22458

**I039** — MemTool 研究：在工具丰富的多轮对话中，通过智能移除策略实现 ≥90% 工具移除效率，同时保持任务完成稳定性。来源：Emergent Mind

**I040** — RAG 场景下上下文管理的挑战：检索文档与对话历史竞争有限的上下文空间。来源：Maxim AI

**I041** — Claude 2.1 初始 Needle in a Haystack 测试仅 27% 准确率，Anthropic 通过优化 prompt 后显著提升。来源：Arize AI

**I042** — 不同 LLM 需要定制化 prompt 策略，微小的 prompt 差异可能导致性能大幅波动。来源：Arize AI

**I043** — Context Engineering 的熵减框架：将人类意图的高熵复杂性压缩为机器可处理的低熵表示。来源：FlowHunt

**I044** — 上下文过载问题：当上下文超过有效容量时，噪声增加，模型利用率下降。来源：JetBrains Research

**I045** — 上下文窗口管理策略：选择性注入、压缩技术、架构模式。来源：Maxim AI

**I046** — Transformer 注意力复杂度随序列长度二次方增长，长上下文处理明显更慢。来源：arxiv.org/abs/2001.08361

**I047** — Claude 提供 1 小时缓存时长（额外费用），默认为 5 分钟。来源：Anthropic Docs

**I048** — Anthropic 实验中 100% 缓存命中率，适合需要可预测延迟的长上下文应用。来源：ngrok Blog

**I049** — Prompt Caching 在 Agent 场景的最佳实践：将动态内容放在 prompt 末尾、系统提示单独缓存、排除动态工具结果。来源：arxiv.org/html/2601.06007v1

**I050** — 朴素全上下文缓存可能适得其反，增加延迟。战略性缓存块控制更重要。来源：PwC Research

---

## 结构化分析

### 分析框架：技术栈分层 + 成本结构

上下文工程是一个多层次的技术栈，每一层都有不同的优化空间和成本考量。

#### 第一层：基础设施层（KV Cache 管理）

**核心问题**：如何高效存储和复用注意力计算的中间结果？

| 技术 | 作用 | 适用场景 |
|------|------|---------|
| KV Cache | 存储每 token 的 K/V 状态，避免重复计算 | 所有生成场景 |
| Attention Sink | 保留初始 token，支持无限序列 | 流式生成、多轮对话 |
| KV Offloading | 将非活跃 cache 移至 CPU/SSD | 长上下文、多用户并发 |

**关键洞察**：I006、I007 显示 KV Cache 是推理瓶颈。128K 上下文消耗 ~40GB HBM，且 60-80% 内存被浪费。优化空间巨大。

#### 第二层：检索层（RAG vs 长上下文）

**核心问题**：如何决定哪些信息进入上下文？

根据 I005、I028、I030 的综合分析，决策框架如下：

| 知识库规模 | 推荐方案 | 理由 |
|-----------|---------|------|
| < 200K tokens | 直接放入 prompt + Caching | 最简单，性能最好 |
| 200K - 2M tokens | 混合：核心知识放 prompt，其他 RAG | 平衡成本和覆盖 |
| > 2M tokens | RAG 为主，长上下文处理查询结果 | 规模经济的必然选择 |

**非共识判断**：主流观点认为长上下文会取代 RAG，但分析显示两者是互补关系。I030 指出 2M 窗口对企业知识库仍然不够；I017 强调 RAG 对获取相关示例的必要性。

#### 第三层：编排层（多轮对话与 Agent）

**核心问题**：如何在多轮交互中保持上下文连贯性？

JetBrains 研究（I019、I020）揭示了两种主要策略：

1. **观察掩码（Observation Masking）**：隐藏旧的、不重要的信息
2. **LLM 摘要**：用 AI 生成压缩版本

**关键发现**：Agent 上下文快速增长，但性能提升有限。这意味着大部分上下文变成了"噪声"而非"信号"。I044 指出上下文过载问题。

#### 第四层：成本优化层（Prompt Caching）

**核心问题**：如何最小化 API 调用成本？

PwC 研究（I012、I049、I050）的三个关键发现：

1. **成本降低 45-80%**：在 Agent 场景下效果显著
2. **TTFT 改善 13-31%**：但需要正确的缓存策略
3. **最佳实践**：动态内容放末尾、系统提示单独缓存、排除工具结果

**定价对比**（I009、I011）：

| 模型 | 正常输入 | 缓存写入 | 缓存读取 | 节省比例 |
|------|---------|---------|---------|---------|
| Claude 3.5 Sonnet | $3.00 | $3.75 | $0.30 | 90% |
| Claude 3 Haiku | $0.25 | $0.30 | $0.03 | 88% |
| Claude 3 Opus | $15.00 | $18.75 | $1.50 | 90% |

---

## 洞见与前瞻

### 非共识判断

**判断 1：长上下文不会杀死 RAG，反而会催生"混合架构"成为主流**

市场认知：Gemini 2M 上下文 → RAG 已死
实际趋势：RAG + 长上下文的组合架构才是最优解

**理由**：
- I030：2M 窗口对知识库仍然不够
- I017：每次请求发送大量 tokens 既浪费又昂贵
- I031：RAG 提供数据聚合、实时更新、隐私控制等不可替代价值

**判断 2：Prompt Caching 的战略价值被低估**

市场认知：Caching 是一个"锦上添花"的优化
实际趋势：Caching 是构建经济可行 Agent 系统的必要条件

**理由**：
- I012：45-80% 成本降低 + 13-31% 延迟改善
- I024：10 轮对话后只支付 10% 正常成本
- I049：战略性缓存控制比简单全缓存更重要

**判断 3：上下文工程正在从"艺术"变为"工程"**

市场认知：Prompt Engineering 是"黑魔法"
实际趋势：正在形成系统化的方法论和最佳实践

**理由**：
- I002：ACE 框架提供了结构化的上下文进化方法
- I026：四大支柱（Composition、Ranking、Optimization、Orchestration）提供了系统视角
- I043：熵减框架将问题形式化

### 前瞻推断（6-12 个月）

**1. 上下文管理工具将涌现**
- 专门做 KV Cache 管理的中间件（如 LMemCache）
- 自动化的上下文压缩和优化服务
- 跨会话的上下文持久化方案

**2. Prompt Caching 定价战**
- 目前 Anthropic 领先，OpenAI 和 Google 将跟进
- 可能出现更长缓存时长（24小时+）的企业级选项
- 细粒度缓存控制 API

**3. 长上下文模型的"有效利用"成为焦点**
- 从"能放多少"转向"该怎么放"
- Needle in a Haystack 类测试成为标准评估
- 新的上下文压缩算法（无损或有损）

**4. Agent 框架内置上下文管理**
- LangChain、AutoGen 等将内置智能上下文策略
- 默认支持 Observation Masking + Summarization
- 上下文使用可视化工具

### 关键不确定性

**1. 硬件演进路径不确定**
- 如果 GPU 内存成本大幅下降，KV Cache 瓶颈可能缓解
- 专用 AI 芯片可能改变架构选择
- **验证信号**：NVIDIA H200/B100 的内存带宽提升幅度

**2. 模型架构变革风险**
- 线性注意力机制可能取代二次方注意力
- State Space Models（如 Mamba）可能改变上下文管理范式
- **验证信号**：新架构在生产环境的采用率

**3. 商业模式演变**
- 按上下文长度定价可能被按价值定价取代
- 缓存可能变成免费的基础设施
- **验证信号**：主要厂商的定价策略调整

**4. 上下文窗口的"有效边界"尚未明确**
- 学术界对"Lost in the Middle"现象的解决程度
- 不同任务类型的最佳上下文长度
- **验证信号**：新的学术研究成果和生产实践

---

## 来源汇总

| 编号 | 来源 | 类型 |
|------|------|------|
| I001-I002 | arxiv.org | 学术论文 |
| I003, I009-I011, I022-I025, I028-I029, I033 | anthropic.com/claude.com | 官方文档/博客 |
| I004 | github.com/gkamradt | 开源项目 |
| I005 | arxiv.org/html/2501.01880v1 | 学术论文 |
| I006 | blog.purestorage.com | 技术博客 |
| I007, I026-I027 | introl.com / codeconductor.ai | 技术博客 |
| I008 | microsoft.com/research | 企业研究 |
| I012, I049-I050 | arxiv.org/html/2601.06007v1 | 学术论文 |
| I013 | cloud.google.com | 官方文档 |
| I014 | siliconflow.com | 技术博客 |
| I015 | developer.nvidia.com | 技术博客 |
| I016 | anthropic.com | 客户案例 |
| I017 | reddit.com | 社区讨论 |
| I018, I040, I045 | getmaxim.ai | 技术博客 |
| I019-I020, I044 | blog.jetbrains.com | 企业研究 |
| I021 | intuitionlabs.ai | 技术分析 |
| I024 | spring.io | 技术博客 |
| I030-I031 | unstructured.io | 技术博客 |
| I032 | arxiv.org/html/2404.12096v1 | 学术论文 |
| I034 | huggingface.co | 技术博客 |
| I035-I036 | bentoml.com | 技术文档 |
| I037 | magazine.sebastianraschka.com | 教程 |
| I038 | arxiv.org/abs/2503.22458 | 学术论文 |
| I039 | emergentmind.com | 研究综述 |
| I041-I042 | arize.com | 技术博客 |
| I043 | flowhunt.io | 技术博客 |
| I046 | arxiv.org/abs/2001.08361 | 学术论文 |
| I047-I048 | docs.anthropic.com / ngrok.com | 官方文档/技术博客 |

---

## 回退记录

无。阶段一信息采集一次性达标，未发生回退。

---

## 采集统计

- **搜索次数**：10 次（3 次被 API 限流）
- **页面获取**：18 次
- **编号信息**：50 条
- **覆盖维度**：技术演进、玩家图谱、用户反馈、成本数据、架构方法论
- **分析框架**：技术栈分层 + 成本结构

---

_报告完成时间：2026-02-15 | 执行时间：约 25 分钟_
